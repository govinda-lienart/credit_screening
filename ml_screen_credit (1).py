# -*- coding: utf-8 -*-
"""ML screen credit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hA-bF5Tps1RQ7hzF5ZDewnwcFbx2P6wb

# MACHINE LEARNING - CREDIT SCREENING

# downloading all required libraries and modules for this script
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import MinMaxScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

"""# Reading and importing the data

The data was generated with chatgpt - no column name as such were generated as it is of common use to privacy with sensitive data, even in simulation.
"""

import pandas as pd
import numpy as np

path = 'https://raw.githubusercontent.com/govinda-lienart/credit_screening/refs/heads/main/synthetic_dataset_with_missing_values.csv'

df = pd.read_csv(path)
display (df)

display (df.head(30))
# seems like the table has some blanks or some values with ?

df.info()

"""# Replace empty values ('') and '?' with NaN"""

(df == '?').sum()
# checking how many values with ?

df.isnull().sum()
# checking how many entries with blanks

df.replace(['', '?'], np.NaN, inplace=True)

"""# Train-Test Split (before separating features and target)

"""

train_df, test_df = train_test_split(df, test_size=0.33, random_state=42)
# here the dataframe df is being split into train_df and test_df in which 33 percent of the data is in the testing dataset.
# the random state is just an arbitary number to have same results when re-running the script

"""# Handle Missing Numeric Values (Impute with Mean)

"""

# Assuming your DataFrame is named 'df'
cols_to_convert = ['A2', 'A3', 'A8', 'A10', 'A14']

# Convert columns to numeric before splitting
for col in cols_to_convert:
    df[col] = pd.to_numeric(df[col], errors='coerce')

# Split the data into training and testing sets
train_df, test_df = train_test_split(df, test_size=0.33, random_state=42)

# Imputation
imputer = SimpleImputer(strategy="mean")
numeric_cols = train_df.select_dtypes(include=[np.number]).columns  # Identify numeric cols
train_df[numeric_cols] = imputer.fit_transform(train_df[numeric_cols])
test_df[numeric_cols] = imputer.transform(test_df[numeric_cols])

df.info()

# nice, the numerical objects were converted into floats (numerical)

df.info()

"""# Handle Missing Categorical Values (Impute with Index - highest frequency value)"""

# Iterate over each column of c_data_train
for col in X_train.columns:
    # Check if the column is of object type (note that the index [0] is just a way to display the most common value accoss the frequency table)
    if X_train[col].dtypes == 'object':
        # Impute with the most frequent value
        X_train = X_train.fillna(X_train[col].value_counts().index[0])
        X_test = X_test.fillna(X_test[col].value_counts().index[0])

"""# One-Hot Encoding for Categorical Features (change the categorical values into boolean value)"""



train_df = pd.get_dummies(train_df, drop_first=True)
test_df = pd.get_dummies(test_df, drop_first=True)

# Display the tables
print("One-Hot Encoded Training Data:")
display(train_df.head(10))  #

print("\nOne-Hot Encoded Testing Data:")
display(test_df)

# i

"""# Ensure test set has the same columns as the training set

"""

test_df = test_df.reindex(columns=train_df.columns, fill_value=0)

"""# Split Features and Target (class_1 contains accepted or rejection credit)

"""

X_train = train_df.drop(columns=["Class_?"])
y_train = train_df["Class_1"]
X_test = test_df.drop(columns=["Class_?"])
y_test = test_df["Class_1"]

"""## Scale Data Using MinMaxScaler"""

scaler = MinMaxScaler(feature_range=(0, 1))
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
y_train = y_train.astype(int)  # Convert True/False to 1/0
y_test = y_test.astype(int)  # Convert True/False to 1/0

X_test_scaled
y_test

"""#  Train Logistic Regression Model

"""

model = LogisticRegression(max_iter=1000)
model.fit(X_train_scaled, y_train)

"""# Evaluating performance with a confusion matrix"""

# Predict instances from the test set
y_pred = model.predict(X_test_scaled)

#  Get the accuracy score
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of Logistic Regression: {accuracy:.4f}")

# Print the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(conf_matrix)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=["Rejected", "Approved"])
disp.plot(cmap="Blues", values_format='d')
plt.title("Confusion Matrix")
plt.show()

"""# Test with New Client Data to see if request for credit is rejected or accepted"""

# 10. Test with New Client Data
new_client_data = {
    "A1": ["b"],       # Example categorical value
    "A2": [40],        # Age
    "A3": [3.5],       # Debt-to-income ratio
    "A4": ["u"],       # Married status
    "A5": ["g"],       # Gender
    "A6": ["cc"],      # Credit Card ownership
    "A7": ["v"],       # Employed
    "A8": [2.5],       # Years of Employment
    "A9": ["f"],       # Prior default
    "A10": [720],      # Credit Score
    "A11": ["t"],      # Drivers License
    "A12": ["t"],      # Citizen status
    "A14": [90000],    # Income
}
new_client_df = pd.DataFrame(new_client_data)
display (new_client_df)

# Preprocess new client data (One-hot encode)  This line performs one-hot encoding
# on the new client's data using the pd.get_dummies() function.
#One-hot encoding converts categorical features (like gender, education level, etc.)
#into numerical representations that the machine learning model can understand.
#drop_first=True is used to avoid redundancy in the encoded features
new_client_encoded = pd.get_dummies(new_client_df, drop_first=True)

display (new_client_encoded)

new_client_encoded = new_client_encoded.reindex(columns=X_train.columns, fill_value=0)
#his line ensures that the columns in the encoded new client data match the columns used during model training (X_train.columns). This is important for consistency.
#If any columns are missing in the new client data, they are added with a fill value of 0
display (new_client_encoded)

new_client_scaled = scaler.transform(new_client_encoded)
display (new_client_scaled)
# This line scales the numerical features in the new client data using the same scaler (scaler) that was used to scale the training data. Scaling ensures that the features have a similar range of values, which can improve the performance of some machine learning models

# Predict for new client
new_client_prediction = model.predict(new_client_scaled)
new_client_probability = model.predict_proba(new_client_scaled)
# This line uses the trained machine learning model (model)
# to predict whether the new client's loan application should be approved or rejected.
#The prediction is stored in new_client_prediction.

# Output the prediction
if new_client_prediction[0] == 1:
    print("New Client Prediction: Approved")
else:
    print("New Client Prediction: Rejected")

